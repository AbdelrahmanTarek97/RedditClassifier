{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageBrowser(URL, browser) :\n",
    "    browser.get(URL)\n",
    "    pageLength = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    htmlBS = BeautifulSoup(browser.page_source,\"html.parser\")\n",
    "    divs = htmlBS.select('div[class*=\"scrollerItem\"]')\n",
    "    checkCounter = 0\n",
    "    noOfDivs = len(divs)\n",
    "    while(noOfDivs< 1200):\n",
    "        lastCheckPoint = pageLength\n",
    "        time.sleep(0.2)\n",
    "        pageLength = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        checkCounter+=1\n",
    "        if checkCounter == 40 :\n",
    "            htmlBS = BeautifulSoup(browser.page_source,\"html.parser\")\n",
    "            divs = htmlBS.select('div[class*=\"scrollerItem\"]')\n",
    "            oldNoOfDivs = noOfDivs\n",
    "            noOfDivs = len(divs)\n",
    "            checkCounter = 0\n",
    "            if oldNoOfDivs == noOfDivs:\n",
    "                return browser\n",
    "    return browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'charmap' codec can't encode character '\\u03b2' in position 146: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2082' in position 185: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2010' in position 133: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2010' in position 127: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u03b1' in position 276: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u202d' in position 2152: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u0107' in position 29: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u200e' in position 4118: character maps to <undefined>\n",
      "'charmap' codec can't encode characters in position 262-265: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2032' in position 133: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2015' in position 100: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u200e' in position 141: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2015' in position 199: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2015' in position 230: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\U0001f605' in position 164: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\U0001f62c' in position 271: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\u2032' in position 331: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\U0001f602' in position 286: character maps to <undefined>\n",
      "'charmap' codec can't encode character '\\ufe0f' in position 739: character maps to <undefined>\n",
      "CSV File Writing Complete\n"
     ]
    }
   ],
   "source": [
    "#Replicating code but this time without the Tag Number, and to write data into csv file\n",
    "#script May scrape only partial data if internet connection is not stable since it will incorrectly think that the browser\n",
    "#has finished loading the page and will go to its next task, thus skipping the page\n",
    "\n",
    "\n",
    "redditsFile = open('Chosen Reddits.txt')\n",
    "browser = webdriver.Chrome()\n",
    "dataFile = open('./data/data.csv','w')\n",
    "fieldnames = ['Text', 'Subreddit','Title']\n",
    "writer = csv.DictWriter(dataFile, fieldnames=fieldnames)\n",
    "\n",
    "\n",
    "for line, classNumber in zip(redditsFile, range(5)):\n",
    "    browser = getPageBrowser(line, browser)\n",
    "    htmlBS = BeautifulSoup(browser.page_source,\"html.parser\")\n",
    "    divs = htmlBS.select('div[class*=\"scrollerItem\"]')\n",
    "    \n",
    "    for div in divs:\n",
    "        try:\n",
    "            text = div.find('div',{\"class\":\"md\"})\n",
    "            title = div.find('h2')\n",
    "            \n",
    "            if text is None :\n",
    "                text = 'none'\n",
    "            else : \n",
    "                text = text.getText()\n",
    "            \n",
    "            if title is None :\n",
    "                title = 'none'\n",
    "            else : \n",
    "                title = title.getText()\n",
    "                \n",
    "            writer.writerow({'Title': title  ,'Text':text, 'Subreddit' : str(classNumber)})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "print('CSV File Writing Complete')\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_punctuation(s):\n",
    "    string_punctuation = \".#,_;:!*$@&[]()<>\\'\\\"\"\n",
    "    clear_string = \"\"\n",
    "    for symbol in s:\n",
    "        if symbol not in string.punctuation and symbol not in string_punctuation:\n",
    "                clear_string += symbol\n",
    "    return clear_string\n",
    "\n",
    "def isGarbage(s):\n",
    "    garbage = ['the','a','an','of','in','from','to','at','as','by']\n",
    "    if s in garbage :\n",
    "        return True\n",
    "    if re.match('http.*',s):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples in the datafile is 2801\n"
     ]
    }
   ],
   "source": [
    "fieldnames = ['Text', 'Subreddit','Title']\n",
    "data = []\n",
    "allWords = []\n",
    "\n",
    "#Collect data from old CSV\n",
    "\n",
    "with open('./data/data.csv') as input :\n",
    "    reader = csv.DictReader(input, fieldnames=fieldnames)\n",
    "    for row in reader : \n",
    "        data.append(row)\n",
    "    print('The total number of samples in the datafile is ' + str(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in our data file is 22675\n",
      "MetaData is now Written in the MetaData.csv file\n"
     ]
    }
   ],
   "source": [
    "#get All Words Throughout all samples\n",
    "\n",
    "for row in data :\n",
    "    text = row['Title']+ row['Text']\n",
    "    words = text.split()\n",
    "    for word in words : \n",
    "        word = clear_punctuation(word)\n",
    "        if len(word)<= 1:\n",
    "            continue\n",
    "        if isGarbage(word) :\n",
    "            continue\n",
    "        if word in allWords :\n",
    "            continue\n",
    "        allWords.append(word)\n",
    "        \n",
    "metaDataFile = open('./data/metaData.csv','w')\n",
    "writer = csv.writer(metaDataFile)\n",
    "writer.writerow(allWords)\n",
    "    \n",
    "print ('The total number of words in our data file is ' + str(len(allWords)))\n",
    "print ('MetaData is now Written in the MetaData.csv file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Writing newData CSV file\n"
     ]
    }
   ],
   "source": [
    "#Rewrite words counts as features in new Data File, and write the target values as well\n",
    "\n",
    "newDataFile = open('./data/newData.csv','w')\n",
    "writer = csv.DictWriter(newDataFile, fieldnames=['Subreddit']+allWords)\n",
    "\n",
    "for row in data : \n",
    "    tempDict = dict()\n",
    "    tempDict['Subreddit'] = row['Subreddit']\n",
    "    text = row['Title']+row['Text']\n",
    "    words = text.split()\n",
    "    for word in words :\n",
    "        word = clear_punctuation(word)\n",
    "        if len(word)<= 1:\n",
    "            continue\n",
    "        if isGarbage(word) :\n",
    "            continue\n",
    "        tempDict[word] = int(tempDict.get(word, 0))+1\n",
    "    for word in allWords:\n",
    "        tempDict[word] = tempDict.get(word, 0)\n",
    "    writer.writerow(tempDict)\n",
    "\n",
    "print('Finished Writing newData CSV file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
